{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8278923,"sourceType":"datasetVersion","datasetId":4916411}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# IMPORTS\nimport os\nimport numpy as np\nimport cv2\nfrom sklearn.metrics import jaccard_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support as prfs\nimport skimage.io as io\nfrom collections import defaultdict\n\n# UNet imports\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms, models\nfrom torch.nn.functional import relu\n\nfrom torch.autograd import Variable\n\n# Custom imports\n\n# tany\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.utils.data\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n\nfrom tqdm import tqdm\nimport random\nimport logging\nimport datetime\nfrom tensorboardX import SummaryWriter\n# import metrics\nimport gc\nfrom pathlib import Path\n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:17:50.402534Z","iopub.execute_input":"2024-05-15T03:17:50.403382Z","iopub.status.idle":"2024-05-15T03:17:50.410928Z","shell.execute_reply.started":"2024-05-15T03:17:50.403349Z","shell.execute_reply":"2024-05-15T03:17:50.409844Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:25:20.412874Z","iopub.execute_input":"2024-05-15T03:25:20.413562Z","iopub.status.idle":"2024-05-15T03:25:21.844881Z","shell.execute_reply.started":"2024-05-15T03:25:20.413529Z","shell.execute_reply":"2024-05-15T03:25:21.843624Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# create output folders\nPath('/kaggle/working/images').mkdir(parents=True, exist_ok=True)\nPath('/kaggle/working/labels').mkdir(parents=True, exist_ok=True)\nPath('/kaggle/working/models').mkdir(parents=True, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:25:23.871759Z","iopub.execute_input":"2024-05-15T03:25:23.872160Z","iopub.status.idle":"2024-05-15T03:25:23.878892Z","shell.execute_reply.started":"2024-05-15T03:25:23.872092Z","shell.execute_reply":"2024-05-15T03:25:23.877812Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# initialize cuda\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:25:28.289293Z","iopub.execute_input":"2024-05-15T03:25:28.289651Z","iopub.status.idle":"2024-05-15T03:25:28.297177Z","shell.execute_reply.started":"2024-05-15T03:25:28.289623Z","shell.execute_reply":"2024-05-15T03:25:28.296102Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"markdown","source":"## ***Deep Learning Methods***\n\n1. UNet\n2. Siamese\n3. SUNet","metadata":{}},{"cell_type":"markdown","source":"### **UNet**","metadata":{}},{"cell_type":"markdown","source":"UNet Architecture\n1. Encoder (Contracting Path): down sampling the input image size while depth increases\n\n    Each Block:\n    - Two 3*3 Convolutional Layers zero-padded with stride=1 Each Followed by a RELU Activation\n    - Max Pooling Layer 2*2 with stride=2 (Dimension halved)(Same Depth) [⬇ Down Sampling] \n\n2. Decoder","metadata":{}},{"cell_type":"markdown","source":"### **Siamese UNet**","metadata":{}},{"cell_type":"markdown","source":"1. Load the dataset using dataloaders","metadata":{}},{"cell_type":"code","source":"# Data Augmentation    \ndef apply_transformations(sample, transformations):\n    image1 = sample['images'][0]\n    image2 = sample['images'][1]\n    label = sample['label']\n    \n    transformed_image1 = transformations(image1)\n    transformed_image2 = transformations(image2)\n    transformed_label = transformations(label)\n    \n    return {'images': (transformed_image1, transformed_image2), 'label': transformed_label}\n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:17:51.523252Z","iopub.execute_input":"2024-05-15T03:17:51.524069Z","iopub.status.idle":"2024-05-15T03:17:51.531244Z","shell.execute_reply.started":"2024-05-15T03:17:51.524034Z","shell.execute_reply":"2024-05-15T03:17:51.530203Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class LoadDataset(Dataset):\n    def __init__(self, input_folder, transforms_list=[], augmentation=False):\n        \n        self.before_folder = os.path.join(input_folder, 'A')\n        self.after_folder = os.path.join(input_folder, 'B')\n        self.label_folder = os.path.join(input_folder, 'label')\n\n        self.file_names = os.listdir(self.before_folder) # any folder msh far2a\n\n        self.transforms = transforms_list\n        \n    def __len__(self):\n        return len(self.file_names)\n\n    def __getitem__(self, idx):\n        before_image = io.imread(os.path.join(self.before_folder, self.file_names[idx]))\n        after_image = io.imread(os.path.join(self.before_folder, self.file_names[idx]))\n        label = io.imread(os.path.join(self.label_folder, self.file_names[idx]))\n\n        \n        label = label.astype('float32')  # Convert to floating point to allow division\n        label = label > 0\n        label = label.astype(np.int64)\n        label = torch.as_tensor(label, dtype=torch.float32)\n        label = label.squeeze()\n\n        if len(self.transforms) == 2:\n            before_image = self.transforms[0](before_image)\n            after_image = self.transforms[1](after_image)\n\n\n        return {'images': (before_image, after_image), 'label': label}\n    \n# Define the transformations\ndata_augmentations = transforms.Compose(\n[transforms.RandomHorizontalFlip(),\ntransforms.RandomVerticalFlip()]\n)\n\ntransform = [transforms.Compose([transforms.ToTensor()]), transforms.Compose([transforms.ToTensor()])]\n\n# Load the dataset\ndataset = LoadDataset('/kaggle/input/sat-dataset/trainval', transform)\n\n# Split the dataset into training, test, and validation sets (80, 10, 10)\ntrain_set, val_set = train_test_split(dataset, test_size=0.2, random_state=42)\n\nprint(\"Length of train set before transformations:\", len(train_set))\n\nnumber_of_augmented_samples = len(train_set)//2\n\nrandom_indices = np.random.choice(np.arange(0, len(train_set)), number_of_augmented_samples, replace=False)\n\nfor i in random_indices:\n    new_sample = apply_transformations(train_set[i], data_augmentations)\n    \n    train_set.append(new_sample)\n\nprint(\"Length of train set after transformations:\", len(train_set))\n    \n# create the DataLoader\ndataloader = {\n    'train': DataLoader(train_set, batch_size=16, shuffle=True),\n    'val': DataLoader(val_set, batch_size=16, shuffle=False),\n}\n\nprint(\"DATASET LOADED\")","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:17:51.533048Z","iopub.execute_input":"2024-05-15T03:17:51.533429Z","iopub.status.idle":"2024-05-15T03:19:52.513514Z","shell.execute_reply.started":"2024-05-15T03:17:51.533399Z","shell.execute_reply":"2024-05-15T03:19:52.512473Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Length of train set before transformations: 3894\nLength of train set after transformations: 5841\nDATASET LOADED\n","output_type":"stream"}]},{"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, gamma=0, alpha=None, size_average=True):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n        if isinstance(alpha, (float, int)):\n            self.alpha = torch.Tensor([alpha, 1-alpha])\n        if isinstance(alpha, list):\n            self.alpha = torch.Tensor(alpha)\n        self.size_average = size_average\n\n    def forward(self, input, target):\n        if input.dim() > 2:\n            # N,C,H,W => N,C,H*W\n            input = input.view(input.size(0), input.size(1), -1)\n\n            # N,C,H*W => N,H*W,C\n            input = input.transpose(1, 2)\n\n            # N,H*W,C => N*H*W,C\n            input = input.contiguous().view(-1, input.size(2))\n\n\n        target = target.view(-1, 1)\n        logpt = F.log_softmax(input,dim=1)\n        logpt = logpt.gather(1, target)\n        logpt = logpt.view(-1)\n        pt = Variable(logpt.data.exp())\n\n        if self.alpha is not None:\n            if self.alpha.type() != input.data.type():\n                self.alpha = self.alpha.type_as(input.data)\n            at = self.alpha.gather(0, target.data.view(-1))\n            logpt = logpt * Variable(at)\n\n        loss = -1 * (1-pt)**self.gamma * logpt\n\n        if self.size_average:\n            return loss.mean()\n        else:\n            return loss.sum()\n\n# Dice Loss for binary classification\ndef dice_loss(y_pred, y_true, smooth=1):\n    y_true_flat = y_true.view(-1)\n    y_pred_flat = y_pred.view(-1)\n    intersection = (y_true_flat * y_pred_flat).sum()\n    return 1 - (2. * intersection + smooth) / (y_true_flat.sum() + y_pred_flat.sum() + smooth)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:19:52.514765Z","iopub.execute_input":"2024-05-15T03:19:52.515044Z","iopub.status.idle":"2024-05-15T03:19:52.527785Z","shell.execute_reply.started":"2024-05-15T03:19:52.515020Z","shell.execute_reply":"2024-05-15T03:19:52.526729Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### **Siamese UNet ECAM**","metadata":{}},{"cell_type":"code","source":"# Model\n \n# The convolution block architecture consists of:\n# 1. Convolution layer with kernel size 3x3 and padding 1 (in_channels, mid_channel)\n# 2. Batch normalization\n# 3. ReLU activation\n# 4. Second convolution layer with kernel size 3x3 and padding 1 (mid_channel, out_channels)\n# 5. Batch normalization\n# 6. ReLU activation of the fist convolution layer with the output from second batch normalization\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, mid_channel, out_channels):\n        super(ConvBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, mid_channel, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(mid_channel)\n        self.conv2 = nn.Conv2d(mid_channel, out_channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True) # activation function (inplace modifies input directly)\n    \n    def forward(self, input):\n        input = self.conv1(input) # first convolution layer\n\n        # save the result of the first convolution for the last layer\n        x = input\n\n        input = self.bn1(input) # first batch normalization\n        input = self.relu(input) # activation function\n\n        input = self.conv2(input) # second convolution layer\n        input = self.bn2(input)\n\n        # add the result of the first convolution to the output of the second convolution\n        input += x\n        output = self.relu(input) # final activation function\n        return output\n\n\n# The channel attention module\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_channels, ratio = 16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        self.fc1 = nn.Conv2d(in_channels,in_channels//ratio,1,bias=False)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Conv2d(in_channels//ratio, in_channels,1,bias=False)\n        self.sigmod = nn.Sigmoid()\n\n    def forward(self,x):\n        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n        out = avg_out + max_out\n        return self.sigmod(out)\n    \n\n# cuild the model\nclass SiameseUNetECAM(nn.Module):\n    def __init__(self, input_channels, output_channels):\n        super(SiameseUNetECAM, self).__init__()\n        torch.nn.Module.dump_patches = True # enables a feature in PyTorch where any changes to the module hierarchy are tracked and patches are dumped to files.\n\n        n1 = 32     # the initial number of channels of feature map\n        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.conv0_0 = ConvBlock(input_channels, filters[0], filters[0])\n        self.conv1_0 = ConvBlock(filters[0], filters[1], filters[1])\n\n        self.Up1_0 = nn.ConvTranspose2d(filters[1], filters[1], 2, stride=2)\n\n        self.conv2_0 = ConvBlock(filters[1], filters[2], filters[2])\n\n        self.Up2_0 = nn.ConvTranspose2d(filters[2], filters[2], 2, stride=2)\n\n        self.conv3_0 = ConvBlock(filters[2], filters[3], filters[3])\n\n        self.Up3_0 = nn.ConvTranspose2d(filters[3], filters[3], 2, stride=2)\n        self.conv4_0 = ConvBlock(filters[3], filters[4], filters[4])\n\n        self.Up4_0 = nn.ConvTranspose2d(filters[4], filters[4], 2, stride=2)\n\n        self.conv0_1 = ConvBlock(filters[0] * 2 + filters[1], filters[0], filters[0])\n        self.conv1_1 = ConvBlock(filters[1] * 2 + filters[2], filters[1], filters[1])\n        self.Up1_1 = nn.ConvTranspose2d(filters[1], filters[1], 2, stride=2)\n        self.conv2_1 = ConvBlock(filters[2] * 2 + filters[3], filters[2], filters[2])\n        self.Up2_1 = nn.ConvTranspose2d(filters[2], filters[2], 2, stride=2)\n        self.conv3_1 = ConvBlock(filters[3] * 2 + filters[4], filters[3], filters[3])\n        self.Up3_1 = nn.ConvTranspose2d(filters[3], filters[3], 2, stride=2)\n\n        self.conv0_2 = ConvBlock(filters[0] * 3 + filters[1], filters[0], filters[0])\n        self.conv1_2 = ConvBlock(filters[1] * 3 + filters[2], filters[1], filters[1])\n        self.Up1_2 = nn.ConvTranspose2d(filters[1], filters[1], 2, stride=2)\n        self.conv2_2 = ConvBlock(filters[2] * 3 + filters[3], filters[2], filters[2])\n        self.Up2_2 = nn.ConvTranspose2d(filters[2], filters[2], 2, stride=2)\n\n        self.conv0_3 = ConvBlock(filters[0] * 4 + filters[1], filters[0], filters[0])\n        self.conv1_3 = ConvBlock(filters[1] * 4 + filters[2], filters[1], filters[1])\n        self.Up1_3 = nn.ConvTranspose2d(filters[1], filters[1], 2, stride=2)\n\n        self.conv0_4 = ConvBlock(filters[0] * 5 + filters[1], filters[0], filters[0])\n\n        self.ca = ChannelAttention(filters[0] * 4, ratio=16)\n        self.ca1 = ChannelAttention(filters[0], ratio=16 // 4)\n\n        self.conv_final = nn.Conv2d(filters[0] * 4, output_channels, kernel_size=1)\n\n        # msh fahma dy beta3mel eh bas mashy ba3deen\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n\n    def forward(self, xA, xB):\n        '''xA'''\n        x0_0A = self.conv0_0(xA)\n        x1_0A = self.conv1_0(self.pool(x0_0A))\n        x2_0A = self.conv2_0(self.pool(x1_0A))\n        x3_0A = self.conv3_0(self.pool(x2_0A))\n        # x4_0A = self.conv4_0(self.pool(x3_0A))\n        '''xB'''\n        x0_0B = self.conv0_0(xB)\n        x1_0B = self.conv1_0(self.pool(x0_0B))\n        x2_0B = self.conv2_0(self.pool(x1_0B))\n        x3_0B = self.conv3_0(self.pool(x2_0B))\n        x4_0B = self.conv4_0(self.pool(x3_0B))\n\n        x0_1 = self.conv0_1(torch.cat([x0_0A, x0_0B, self.Up1_0(x1_0B)], 1))\n        x1_1 = self.conv1_1(torch.cat([x1_0A, x1_0B, self.Up2_0(x2_0B)], 1))\n        x0_2 = self.conv0_2(torch.cat([x0_0A, x0_0B, x0_1, self.Up1_1(x1_1)], 1))\n\n\n        x2_1 = self.conv2_1(torch.cat([x2_0A, x2_0B, self.Up3_0(x3_0B)], 1))\n        x1_2 = self.conv1_2(torch.cat([x1_0A, x1_0B, x1_1, self.Up2_1(x2_1)], 1))\n        x0_3 = self.conv0_3(torch.cat([x0_0A, x0_0B, x0_1, x0_2, self.Up1_2(x1_2)], 1))\n\n        x3_1 = self.conv3_1(torch.cat([x3_0A, x3_0B, self.Up4_0(x4_0B)], 1))\n        x2_2 = self.conv2_2(torch.cat([x2_0A, x2_0B, x2_1, self.Up3_1(x3_1)], 1))\n        x1_3 = self.conv1_3(torch.cat([x1_0A, x1_0B, x1_1, x1_2, self.Up2_2(x2_2)], 1))\n        x0_4 = self.conv0_4(torch.cat([x0_0A, x0_0B, x0_1, x0_2, x0_3, self.Up1_3(x1_3)], 1))\n\n        output = torch.cat([x0_1, x0_2, x0_3, x0_4], 1)\n\n        intra = torch.sum(torch.stack((x0_1, x0_2, x0_3, x0_4)), dim=0)\n        ca1 = self.ca1(intra)\n        output = self.ca(output) * (output + ca1.repeat(1, 4, 1, 1))\n        output = self.conv_final(output)\n\n        return (output, )","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:19:52.529655Z","iopub.execute_input":"2024-05-15T03:19:52.530038Z","iopub.status.idle":"2024-05-15T03:19:52.570671Z","shell.execute_reply.started":"2024-05-15T03:19:52.530007Z","shell.execute_reply":"2024-05-15T03:19:52.569746Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.padding import ReplicationPad2d\n\nclass SiameseUNet(nn.Module):\n    \"\"\"SiamUnet_diff segmentation network.\"\"\"\n\n    def __init__(self, input_nbr, label_nbr):\n        super(SiameseUNet, self).__init__()\n\n        self.input_nbr = input_nbr\n\n        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\n        self.bn11 = nn.BatchNorm2d(16)\n        self.do11 = nn.Dropout2d(p=0.2)\n        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n        self.bn12 = nn.BatchNorm2d(16)\n        self.do12 = nn.Dropout2d(p=0.2)\n\n        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        self.bn21 = nn.BatchNorm2d(32)\n        self.do21 = nn.Dropout2d(p=0.2)\n        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n        self.bn22 = nn.BatchNorm2d(32)\n        self.do22 = nn.Dropout2d(p=0.2)\n\n        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn31 = nn.BatchNorm2d(64)\n        self.do31 = nn.Dropout2d(p=0.2)\n        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn32 = nn.BatchNorm2d(64)\n        self.do32 = nn.Dropout2d(p=0.2)\n        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn33 = nn.BatchNorm2d(64)\n        self.do33 = nn.Dropout2d(p=0.2)\n\n        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn41 = nn.BatchNorm2d(128)\n        self.do41 = nn.Dropout2d(p=0.2)\n        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn42 = nn.BatchNorm2d(128)\n        self.do42 = nn.Dropout2d(p=0.2)\n        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn43 = nn.BatchNorm2d(128)\n        self.do43 = nn.Dropout2d(p=0.2)\n\n        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n        self.bn43d = nn.BatchNorm2d(128)\n        self.do43d = nn.Dropout2d(p=0.2)\n        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n        self.bn42d = nn.BatchNorm2d(128)\n        self.do42d = nn.Dropout2d(p=0.2)\n        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n        self.bn41d = nn.BatchNorm2d(64)\n        self.do41d = nn.Dropout2d(p=0.2)\n\n        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n        self.bn33d = nn.BatchNorm2d(64)\n        self.do33d = nn.Dropout2d(p=0.2)\n        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n        self.bn32d = nn.BatchNorm2d(64)\n        self.do32d = nn.Dropout2d(p=0.2)\n        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n        self.bn31d = nn.BatchNorm2d(32)\n        self.do31d = nn.Dropout2d(p=0.2)\n\n        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n        self.bn22d = nn.BatchNorm2d(32)\n        self.do22d = nn.Dropout2d(p=0.2)\n        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n        self.bn21d = nn.BatchNorm2d(16)\n        self.do21d = nn.Dropout2d(p=0.2)\n\n        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n        self.bn12d = nn.BatchNorm2d(16)\n        self.do12d = nn.Dropout2d(p=0.2)\n        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n\n        self.sm = nn.LogSoftmax(dim=1)\n\n    def forward(self, x1, x2):\n\n\n        \"\"\"Forward method.\"\"\"\n        # for imput image 1\n        # Stage 1\n        x11 = self.do11(F.relu(self.bn11(self.conv11(x1))))\n        x12_1 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n        x1p = F.max_pool2d(x12_1, kernel_size=2, stride=2)\n\n\n        # Stage 2\n        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n        x22_1 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n        x2p = F.max_pool2d(x22_1, kernel_size=2, stride=2)\n\n        # Stage 3\n        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n        x33_1 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n        x3p = F.max_pool2d(x33_1, kernel_size=2, stride=2)\n\n        # Stage 4\n        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n        x43_1 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n        x4p = F.max_pool2d(x43_1, kernel_size=2, stride=2)\n\n        ####################################################\n        # for input image 2\n        # Stage 1\n        x11 = self.do11(F.relu(self.bn11(self.conv11(x2))))\n        x12_2 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n        x1p = F.max_pool2d(x12_2, kernel_size=2, stride=2)\n\n\n        # Stage 2\n        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n        x22_2 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n        x2p = F.max_pool2d(x22_2, kernel_size=2, stride=2)\n\n        # Stage 3\n        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n        x33_2 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n        x3p = F.max_pool2d(x33_2, kernel_size=2, stride=2)\n\n        # Stage 4\n        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n        x43_2 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n        x4p = F.max_pool2d(x43_2, kernel_size=2, stride=2)\n\n\n\n        # Stage 4d\n        x4d = self.upconv4(x4p)\n        pad4 = ReplicationPad2d((0, x43_1.size(3) - x4d.size(3), 0, x43_1.size(2) - x4d.size(2)))\n        x4d = torch.cat((pad4(x4d), torch.abs(x43_1 - x43_2)), 1)\n        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n\n        # Stage 3d\n        x3d = self.upconv3(x41d)\n        pad3 = ReplicationPad2d((0, x33_1.size(3) - x3d.size(3), 0, x33_1.size(2) - x3d.size(2)))\n        x3d = torch.cat((pad3(x3d), torch.abs(x33_1 - x33_2)), 1)\n        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n\n        # Stage 2d\n        x2d = self.upconv2(x31d)\n        pad2 = ReplicationPad2d((0, x22_1.size(3) - x2d.size(3), 0, x22_1.size(2) - x2d.size(2)))\n        x2d = torch.cat((pad2(x2d), torch.abs(x22_1 - x22_2)), 1)\n        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n\n        # Stage 1d\n        x1d = self.upconv1(x21d)\n        pad1 = ReplicationPad2d((0, x12_1.size(3) - x1d.size(3), 0, x12_1.size(2) - x1d.size(2)))\n        x1d = torch.cat((pad1(x1d), torch.abs(x12_1 - x12_2)), 1)\n        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n        x11d = self.conv11d(x12d)\n\n        return self.sm(x11d)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:19:52.572215Z","iopub.execute_input":"2024-05-15T03:19:52.572610Z","iopub.status.idle":"2024-05-15T03:19:52.619068Z","shell.execute_reply.started":"2024-05-15T03:19:52.572577Z","shell.execute_reply":"2024-05-15T03:19:52.618088Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# some functions and definitions for training\nparameters = {\n  \"patch_size\": 256,\n  \"epochs\": 20,\n  \"batch_size\": 16,\n  \"learning_rate\": 1e-3,\n  \"dataset_dir\": \"./dataset/trainval/\",\n}\n\ntrain_set = dataloader['train']\nval_set = dataloader['val']\n\ndef seed_torch(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n\ndef initialize_metrics():\n    \"\"\"Generates a dictionary of metrics with metrics as keys\n       and empty lists as values\n\n    Returns\n    -------\n    dict\n        a dictionary of metrics\n\n    \"\"\"\n    metrics = {\n        'cd_losses': [],\n        'cd_corrects': [],\n        'cd_precisions': [],\n        'cd_recalls': [],\n        'cd_f1scores': [],\n        'learning_rate': [],\n        'jaccard_scores': []\n    }\n\n    return metrics\n\ndef set_metrics(metric_dict, cd_loss, cd_corrects, cd_report, lr, jaccard_score):\n    \"\"\"Updates metric dict with batch metrics\n\n    Parameters\n    ----------\n    metric_dict : dict\n        dict of metrics\n    cd_loss : dict(?)\n        loss value\n    cd_corrects : dict(?)\n        number of correct results (to generate accuracy\n    cd_report : list\n        precision, recall, f1 values\n\n    Returns\n    -------\n    dict\n        dict of  updated metrics\n\n\n    \"\"\"\n    metric_dict['cd_losses'].append(cd_loss.item())\n    metric_dict['cd_corrects'].append(cd_corrects.item())\n    metric_dict['cd_precisions'].append(cd_report[0])\n    metric_dict['cd_recalls'].append(cd_report[1])\n    metric_dict['cd_f1scores'].append(cd_report[2])\n    metric_dict['learning_rate'].append(lr)\n    metric_dict['jaccard_scores'].append(jaccard_score)\n\n    return metric_dict\n\n\n\ndef get_mean_metrics(metric_dict):\n    \"\"\"takes a dictionary of lists for metrics and returns dict of mean values\n\n    Parameters\n    ----------\n    metric_dict : dict\n        A dictionary of metrics\n\n    Returns\n    -------\n    dict\n        dict of floats that reflect mean metric value\n\n    \"\"\"\n    return {k: np.mean(v) for k, v in metric_dict.items()}\n\n\ndef hybrid_loss(predictions, targets):\n    \"\"\"Calculating the loss\"\"\"\n    loss = 0\n\n    # gamma=0, alpha=None --> CE\n#     focal = FocalLoss(gamma=0, alpha=None)\n    bceLoss = nn.BCELoss()\n\n    for prediction, target in zip(predictions, targets):\n        \n        bce = bceLoss(prediction.float(), target.float())\n        dice = dice_loss(torch.sigmoid(prediction), target)\n        loss += bce + dice\n\n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:24:43.713632Z","iopub.execute_input":"2024-05-15T03:24:43.713993Z","iopub.status.idle":"2024-05-15T03:24:43.727015Z","shell.execute_reply.started":"2024-05-15T03:24:43.713965Z","shell.execute_reply":"2024-05-15T03:24:43.726052Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def evaluate(model):\n    model.eval()\n    \n    jaccard_scores = []\n    micro_jaccard_scores = []\n    \n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    i = 0\n    with torch.no_grad():\n        tbar = tqdm(val_set)\n        batch_iteration = 0\n        for batch in tbar:\n            tbar.set_description(f\"batch {batch_iteration} info \" + \" - \" + str(batch_iteration + parameters['batch_size']))\n\n            # load the data to the device\n            before_images = batch['images'][0].to(device)\n            after_images = batch['images'][1].to(device)\n            labels = batch['label'].long().to(device)\n            \n            \n            predictions = model(before_images, after_images)\n            _, predictions = torch.max(predictions, 1)\n            \n            for prediction, label in zip(predictions, labels):\n                predicted = prediction.cpu().numpy().astype(np.uint8)\n                \n                ground_truth = label.cpu().numpy().astype(np.uint8)\n                \n                if i == 0:\n                    print(\"Ground Truth:\", ground_truth)\n                    print(\"Predicted:\", predicted)\n                \n                # calculate jaccard score\n                \n                jaccard_scores.append(jaccard_score(predicted.flatten(), ground_truth.flatten(), zero_division=1))\n                \n                micro_jaccard_scores.append(jaccard_score(predicted.flatten(), ground_truth.flatten(), zero_division=1, average='micro'))\n                \n                # Save images if needed\n                cv2.imwrite(\"/kaggle/working/images/\" + f'{i}.jpg', predicted.reshape(256, 256, 1) * 255)\n                cv2.imwrite(\"/kaggle/working/labels/\" + f'{i}.jpg', ground_truth.reshape(256, 256, 1) * 255)\n                \n                i += 1\n            \n            del before_images, after_images, labels\n            \n        jaccard_mean = np.mean(jaccard_scores)\n        micro_jaccard_mean = np.mean(micro_jaccard_scores)\n            \n        print(\"Jaccard Mean:\", jaccard_mean)\n        print(\"Micro Jaccard Mean:\", micro_jaccard_mean)\n        \n        return jaccard_mean, micro_jaccard_mean\n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:19:52.639018Z","iopub.execute_input":"2024-05-15T03:19:52.639403Z","iopub.status.idle":"2024-05-15T03:19:52.654718Z","shell.execute_reply.started":"2024-05-15T03:19:52.639377Z","shell.execute_reply":"2024-05-15T03:19:52.653905Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# train the model\nseed_torch(seed=777)\n\n\n\"\"\"\nLoad Model then define other aspects of the model\n\"\"\"\n\nmodel = SiameseUNet(3,2).to(device)\n\n# Loss function\n# criterion = hybrid_loss # loss function bce + dice\n# criterion = hybrid_loss\n# criterion = nn.NLLLoss()\ncriterion = torch.nn.NLLLoss(weight=torch.tensor([0.2,0.8]).to(device))\noptimizer = torch.optim.AdamW(model.parameters(), lr=parameters['learning_rate']) # Be careful when you adjust learning rate, you can refer to the linear scaling rule\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.5)\n\n\"\"\"\n Set starting values\n\"\"\"\n# best_metrics = {'cd_f1scores': -1, 'cd_recalls': -1, 'cd_precisions': -1}\n# logging.info('STARTING training')\ntotal_step = -1\n\nvalidation_jaccard = float('-inf')\n\n# training loop\nfor epoch in range(parameters['epochs']):\n    epoch_loss = []\n    \n    model.train()\n\n    batch_iteration = 0\n\n    tbar = tqdm(train_set)\n    for batch in tbar:\n        tbar.set_description(\"epoch {} info \".format(epoch) + str(batch_iteration) + \" - \" + str(batch_iteration + parameters['batch_size']))\n        batch_iteration = batch_iteration + parameters['batch_size']\n        total_step += 1\n\n        # load the data to the device\n        before_images = batch['images'][0].to(device)\n        after_images = batch['images'][1].to(device)\n        labels = batch['label'].long().to(device)\n\n        \n        # Zero the gradient\n        optimizer.zero_grad()\n\n        # Get model predictions, calculate loss, backprop\n        predictions = model(before_images, after_images)\n        \n\n\n        # calculate the loss\n        cd_loss = criterion(predictions, labels)\n        \n        loss = cd_loss\n\n        # backpropagation\n        loss.backward()\n        optimizer.step()        \n        \n        _, predictions = torch.max(predictions, 1)\n\n    \n        epoch_loss.append(loss.item())\n\n        del before_images, after_images, labels\n    \n    scheduler.step()\n    \n    current_loss=sum(epoch_loss)/len(epoch_loss)\n    gc.collect()\n    \n    jaccard_test, micro_jaccard_test = evaluate(model)\n    \n    torch.save(model.state_dict(), f\"/kaggle/working/models/pretrained_epoch_{epoch}.pth\")\n    \n    if jaccard_test > validation_jaccard:\n        torch.save(model.state_dict(), f\"/kaggle/working/models/best_model.pth\")\n        validation_jaccard = jaccard_test\n\n    print(f'Epoch {epoch} finished.')\n    \n    \nprint('Done!')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:25:37.340059Z","iopub.execute_input":"2024-05-15T03:25:37.340913Z","iopub.status.idle":"2024-05-15T04:11:44.952660Z","shell.execute_reply.started":"2024-05-15T03:25:37.340881Z","shell.execute_reply":"2024-05-15T04:11:44.951601Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"epoch 0 info 5840 - 5856: 100%|██████████| 366/366 [01:37<00:00,  3.77it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6837799812592703\nMicro Jaccard Mean: 0.9003238672559277\nEpoch 0 finished.\n","output_type":"stream"},{"name":"stderr","text":"epoch 1 info 5840 - 5856: 100%|██████████| 366/366 [01:36<00:00,  3.78it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6899383983572895\nMicro Jaccard Mean: 0.9003468088001243\nEpoch 1 finished.\n","output_type":"stream"},{"name":"stderr","text":"epoch 2 info 5840 - 5856: 100%|██████████| 366/366 [01:36<00:00,  3.78it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6899383983572895\nMicro Jaccard Mean: 0.9003468088001243\nEpoch 2 finished.\n","output_type":"stream"},{"name":"stderr","text":"epoch 3 info 5840 - 5856: 100%|██████████| 366/366 [01:36<00:00,  3.78it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6899383983572895\nMicro Jaccard Mean: 0.9003468088001243\nEpoch 3 finished.\n","output_type":"stream"},{"name":"stderr","text":"epoch 4 info 5840 - 5856: 100%|██████████| 366/366 [01:36<00:00,  3.78it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6899383983572895\nMicro Jaccard Mean: 0.9003468088001243\nEpoch 4 finished.\n","output_type":"stream"},{"name":"stderr","text":"epoch 5 info 5840 - 5856: 100%|██████████| 366/366 [01:36<00:00,  3.78it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6899383983572895\nMicro Jaccard Mean: 0.9003468088001243\nEpoch 5 finished.\n","output_type":"stream"},{"name":"stderr","text":"epoch 6 info 5840 - 5856: 100%|██████████| 366/366 [01:36<00:00,  3.78it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6899383983572895\nMicro Jaccard Mean: 0.9003468088001243\nEpoch 6 finished.\n","output_type":"stream"},{"name":"stderr","text":"epoch 7 info 5840 - 5856: 100%|██████████| 366/366 [01:36<00:00,  3.78it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6899383983572895\nMicro Jaccard Mean: 0.9003468088001243\nEpoch 7 finished.\n","output_type":"stream"},{"name":"stderr","text":"epoch 8 info 5840 - 5856: 100%|██████████| 366/366 [01:36<00:00,  3.78it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6899383983572895\nMicro Jaccard Mean: 0.9003468088001243\nEpoch 8 finished.\n","output_type":"stream"},{"name":"stderr","text":"epoch 9 info 5840 - 5856: 100%|██████████| 366/366 [01:36<00:00,  3.78it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6899383983572895\nMicro Jaccard Mean: 0.9003468088001243\nEpoch 9 finished.\n","output_type":"stream"},{"name":"stderr","text":"epoch 10 info 5840 - 5856: 100%|██████████| 366/366 [01:36<00:00,  3.78it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6899383983572895\nMicro Jaccard Mean: 0.9003468088001243\nEpoch 10 finished.\n","output_type":"stream"},{"name":"stderr","text":"epoch 11 info 5840 - 5856: 100%|██████████| 366/366 [01:36<00:00,  3.78it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6899383983572895\nMicro Jaccard Mean: 0.9003468088001243\nEpoch 11 finished.\n","output_type":"stream"},{"name":"stderr","text":"epoch 12 info 5840 - 5856: 100%|██████████| 366/366 [01:36<00:00,  3.78it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6899383983572895\nMicro Jaccard Mean: 0.9003468088001243\nEpoch 12 finished.\n","output_type":"stream"},{"name":"stderr","text":"epoch 13 info 5840 - 5856: 100%|██████████| 366/366 [01:36<00:00,  3.79it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6899383983572895\nMicro Jaccard Mean: 0.9003468088001243\nEpoch 13 finished.\n","output_type":"stream"},{"name":"stderr","text":"epoch 14 info 5840 - 5856: 100%|██████████| 366/366 [01:36<00:00,  3.78it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6899383983572895\nMicro Jaccard Mean: 0.9003468088001243\nEpoch 14 finished.\n","output_type":"stream"},{"name":"stderr","text":"epoch 15 info 5840 - 5856: 100%|██████████| 366/366 [01:36<00:00,  3.78it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6899383983572895\nMicro Jaccard Mean: 0.9003468088001243\nEpoch 15 finished.\n","output_type":"stream"},{"name":"stderr","text":"epoch 16 info 5840 - 5856: 100%|██████████| 366/366 [01:36<00:00,  3.78it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6899383983572895\nMicro Jaccard Mean: 0.9003468088001243\nEpoch 16 finished.\n","output_type":"stream"},{"name":"stderr","text":"epoch 17 info 5840 - 5856: 100%|██████████| 366/366 [01:36<00:00,  3.78it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6899383983572895\nMicro Jaccard Mean: 0.9003468088001243\nEpoch 17 finished.\n","output_type":"stream"},{"name":"stderr","text":"epoch 18 info 5840 - 5856: 100%|██████████| 366/366 [01:36<00:00,  3.78it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6899383983572895\nMicro Jaccard Mean: 0.9003468088001243\nEpoch 18 finished.\n","output_type":"stream"},{"name":"stderr","text":"epoch 19 info 5840 - 5856: 100%|██████████| 366/366 [01:36<00:00,  3.78it/s]\nbatch 0 info  - 16:   0%|          | 0/61 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Ground Truth: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\nPredicted: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"name":"stderr","text":"batch 0 info  - 16: 100%|██████████| 61/61 [00:41<00:00,  1.49it/s]","output_type":"stream"},{"name":"stdout","text":"Jaccard Mean: 0.6899383983572895\nMicro Jaccard Mean: 0.9003468088001243\nEpoch 19 finished.\nDone!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# show random images\nimport random\nimport matplotlib.pyplot as plt\nimport skimage.io as io\nimages_filenames = os.listdir(\"/kaggle/working/images/\")\n\nrandom_filename = random.choice(images_filenames) \n\nimage = io.imread(\"/kaggle/working/images/\"+random_filename)\n\n# print(image)\n\nio.imshow(image)\nio.show()\n\nlabel = io.imread(\"/kaggle/working/labels/\"+random_filename)\n\nio.imshow(label)\nio.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T03:19:52.682584Z","iopub.status.idle":"2024-05-15T03:19:52.682987Z","shell.execute_reply.started":"2024-05-15T03:19:52.682807Z","shell.execute_reply":"2024-05-15T03:19:52.682822Z"},"trusted":true},"execution_count":null,"outputs":[]}]}